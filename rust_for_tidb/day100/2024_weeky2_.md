

TiKV RocksDB读写原理笔记



原文：https://tidb.net/blog/614b0fe3

下面对该文章的个人理解：





RocksDB 空间占用



多版本：

RocksDB 作为一个 LSM-tree 结构的键值存储引擎，MemTable 中的数据会首先被刷到 L0。

L0 层的 SST 之间的范围可能存在重叠（因为文件顺序是按照生成的顺序排列），因此同一个 key 在 L0 中可能存在多个版本。

当文件从 L0 合并到 L1 的时候，会按照一定大小（默认是 8MB）切割为多个文件，同一层的文件的范围互不重叠，

所以 L1 及其以后的层每一层的 key 都只有一个版本。





TiKV 的空间放大：

TiKV 在 RocksDB 之上还有一层自己的 MVCC，当用户写入一个 key 的时候，实际上写入到 RocksDB 的是 key + commit_ts，也就是说，用户的更新和删除都是会写入新的 key 到 RocksDB。

TiKV 每隔一段时间会删除旧版本的数据（通过 RocksDB 的 Delete 接口），因此可以认为用户存储在 TiKV 上的数据的实际空间放大为，1.11 加最近 10 分钟内写入的数据（假设 TiKV 回收旧版本数据足够及时）。详情见[《TiDB in Action》](https://github.com/pingcap-incubator/tidb-in-action/blob/master/session4/chapter7/compact.md#tikv-的空间放大)。



LSM-Tree写入流程图

![](https://tidb-blog.oss-cn-beijing.aliyuncs.com/media/image-1675676362330.png)



LSM-Tree读取流程图

![](https://tidb-blog.oss-cn-beijing.aliyuncs.com/media/image-1675676381841.png)

RocksDB写入流程

2.1 写入流程

1、产生写入请求(put\delete) 

​      TiKV-Details -> RocksDB KV/RocksDB raft -> **Write operations** (正常情况下with_wal、done 的数量应该是保持一致)

2、写入操作系统缓存。如果配置sync-log=true，则同时执行刷盘操作fsync写入本地文件

​      先写 WAL 日志文件，方便 crash recovery 的时候可以根据日志恢复。配置sync-log=false，把数据写进了操作系统的缓存区就返回了，进行下一步

​      TiKV-Details -> RocksDB KV/RocksDB raft -> **Write WAL duration (**在进行 WAL 时所花费的时间**)**

​      TiKV-Details -> RocksDB KV/RocksDB raft -> **WAL sync operations** (调用操作系统 fsync 的次数)

​      TiKV-Details -> RocksDB KV/RocksDB raft -> **WAL sync duration** (调用操作系统 fsync 将数据持久化到硬盘上耗时)

3、将请求写入到 memtable 中，并返回写入成功信息给客户端。数据后台进行compact

​      TiKV-Details -> RocksDB KV/RocksDB raft -> **Write Durtion** (收到 put/delete 请求到完成请求返回给 client 所花费的时间)

至此数据已经写入完毕并返回客户端执行成功，剩下的就是flush与compact操作





2.2 memtable flush

当一个 MemTable 的大小超过 128MB 时，会切换到一个新的 MemTable 来提供写入。

写满之后的转化为immutable，然后进行刷盘flush。

当达到 memtable 最大个数限制，就会触发 RocksDB 的 write stall 。

```markdown
### 可配置到 rocksdb.writecf 、rocksdb.defaultcf
rocksdb.writecf.write-buffer-size: 256MB              # memtable 大小
rocksdb.writecf.min-write-buffer-number-to-merge: 1   # immutable 达到多少个则进行刷盘flush
rocksdb.writecf.max-write-buffer-number: 24           # memtable 最大个数
rocksdb.writecf.max-background-flushes: 2             # memtable 刷写的最大后台线程数
```

TiKV-Details -> RocksDB KV/RocksDB raft -> Write Stall Reason 或者 RocksDB 日志（查找 Stalling 关键字）确认是否是 level0 sst 文件过多导致 write stall



2.3 immutable compaction 到 L0 层

immutable 数量达到 min-write-buffer-number-to-merge 之后就会触发 flush。

L0 层上包含的文件，是由内存中的memtable dump到磁盘上生成的，单个文件内部按key有序，文件之间无序。可能存在多个相同的key在 L0 层

L1~L6层上的文件都是按照key有序的。也就是每层只会存在一个key。



**SST 文件命名格式**

SST 文件以 `storeID_regionID_regionEpoch_keyHash_cf` 的格式命名。格式名的解释如下：

- storeID：TiKV 节点编号
- regionID：Region 编号
- regionEpoch：Region 版本号
- keyHash：Range startKey 的 Hash (sha256) 值，确保唯一性
- cf：RocksDB 的 ColumnFamily（默认为 `default` 或 `write`）





2.4 L0 层 compaction 到 L1 层

```markdown
### 可配置到 rocksdb.writecf 、rocksdb.defaultcf
rocksdb.writecf.level0-file-num-compaction-trigger: 4 # 触发 L0 向 L1 合并的 L0 文件数
rocksdb.writecf.level0-slowdown-writes-trigger: 32    # 触发 write stall 的 L0 文件数
rocksdb.writecf.level0-stop-writes-trigger: 64        # 触发完全阻塞写入的 L0 文件数

### 向 L1 的compaction不可以与其他level compaction并行。需单独配置此参数
rocksdb.max-sub-compactions: 2
```



2.5 L1~L6层 compaction

L1~Ln 层是否需要 Compaction 是依据每一层 SST 文件大小是否超过阈值。可根据以下参数进行配置。

```markdown
### 可配置到 rocksdb.writecf 、rocksdb.defaultcf
rocksdb.writecf.target-file-size-base: 8MB            # SSTable 文件大小，文件从 L0 合并到 L1
rocksdb.writecf.max-bytes-for-level-base: 512MB       # base LEVEL (L1) 最大字节数，一般设置为 memtable 大小 4 倍
rocksdb.writecf.max-bytes-for-level-multiplier: 10    # 每一层的默认放大倍数。默认值 ： 10
rocksdb.writecf.num-levels: 7                         # 文件最大层数。默认值 ： 7
rocksdb.writecf.compression-per-level: ["no","no","lz4","lz4","lz4","zstd","zstd"]   # 每层压缩算法
```

#### 2.5.1 Compaction 策略

每层大小当超过以下阈值时则会进行 Compaction ，把数据合并到下一层。

| Level | L1    | L2   | L3   | L4    | L5   | L6   |      |
| ----- | ----- | ---- | ---- | ----- | ---- | ---- | ---- |
| Size  | 512MB | 5GB  | 50GB | 500GB | 5TB  | 50TB |      |

当多个 Level 都满足触发Compaction的条件，该如何选择？

- 对于L1-L6，score = 该level文件的总长度 / 阈值。已经正在做Compaction的文件不计入总长度中
- 对于L0，score = max{文件数量 / level0-file-num-compaction-trigger， L0文件总长度 / max-bytes-for-level-base} 并且 L0文件数量 > level0-file-num-compaction-trigger







# TiKV 的读取请求分为两类

- 一类是指定查询某一行或者某几行的简单查询，这类查询会运行在 Storage Read Pool 中。



- 另一类是复杂的聚合计算、范围查询，这类请求会运行在 Coprocessor Read Pool 中。



从 TiKV 5.0 版本起，默认所有的读取请求都通过统一的线程池进行查询。

从 TiKV 4.0 升级上来的 TiKV 集群且升级前未打开 readpool.storage 的 use-unified-pool 配置，

则升级后所有的读取请求仍然继续使用独立的线程池进行查询，

可以将 readpool.storage.use-unified-pool 设置为 true 使所有的读取请求通过统一的线程池进行查询。



在 UnifyRead Pool 线程池，读取操作将分为3个不同的优先级L0、L1、L2，执行快占用资源少的将会优先执行。在集群负载较高的时候，会发现一些慢SQL执行的更慢了。

